{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5071a1",
   "metadata": {},
   "source": [
    "## DistilBERT Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f089b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import helpers as helpers\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0488d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT: max_len=128–256, batch_size=16, lr≈2e-5, epochs=2–3, early stopping.\n",
    "\n",
    "DEPRESSION_PATH = \"data/depression_dataset_reddit_cleaned.csv\"\n",
    "TDT_SPLIT = \"80/10/10\"\n",
    "BERT_MODEL = 'distilbert-base-uncased'\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e686069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing 80/10/10 split\n",
      "Completing 80/10/10 split\n"
     ]
    }
   ],
   "source": [
    "data, labels = helpers.get_data()\n",
    "train_text, dev_text, test_text = helpers.split(data=data, dist=TDT_SPLIT)\n",
    "train_dep, dev_dep, test_dep = helpers.split(data=labels, dist=TDT_SPLIT)\n",
    "\n",
    "# convert data, labels into a pandas DataFrame\n",
    "df = pd.DataFrame({'clean_text': data, 'is_depression': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7ffd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL)\n",
    "bert = DistilBertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ffeec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length is :  30522\n",
      "network\n",
      "first 10: ['[PAD]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]']\n",
      "1280-1290: ['ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ']\n",
      "last 10: ['##！', '##（', '##）', '##，', '##－', '##．', '##／', '##：', '##？', '##～']\n",
      "token 2897: network\n",
      "token 102: [SEP]\n",
      "token 103: [MASK]\n",
      "alex id: 4074\n"
     ]
    }
   ],
   "source": [
    "# stats from tokenizers\n",
    "print(\"Vocab length is : \",len(tokenizer.vocab))\n",
    "print(tokenizer.decode(2897))\n",
    "\n",
    "vocab = list(tokenizer.vocab.keys())\n",
    "print(f\"first 10: {vocab[:10]}\")\n",
    "print(f\"1280-1290: {vocab[1280:1290]}\")\n",
    "print(f\"last 10: {vocab[-10:]}\")\n",
    "\n",
    "print(f\"token 2897: {tokenizer.decode(2897)}\")\n",
    "print(f\"token 102: {tokenizer.decode(102)}\")\n",
    "print(f\"token 103: {tokenizer.decode(103)}\")\n",
    "if 'alex' in tokenizer.vocab:\n",
    "  print(f\"alex id: {tokenizer.vocab['alex']}\")\n",
    "else:\n",
    "  print(\"alex not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482facdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize train, dev, and test\n",
    "train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "dev_encodings = tokenizer(dev_text, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e855a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_dep)\n",
    "dev_labels = torch.tensor(dev_dep)\n",
    "test_labels = torch.tensor(test_dep)\n",
    "\n",
    "# make TensorDatasets\n",
    "train_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'],\n",
    "    train_encodings['attention_mask'],\n",
    "    train_labels\n",
    ")\n",
    "\n",
    "dev_dataset = TensorDataset(\n",
    "    dev_encodings['input_ids'],\n",
    "    dev_encodings['attention_mask'],\n",
    "    dev_labels\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'],\n",
    "    test_encodings['attention_mask'],\n",
    "    test_labels\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "dl_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_dev = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dl_test = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e74de0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e513ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# begin training\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
