{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5071a1",
   "metadata": {},
   "source": [
    "## DistilBERT Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/griffinmichalak/Desktop/code/fall25/nlp/nlp-final-project/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/griffinmichalak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import helpers as helpers\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0488d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT: max_len=128–256, batch_size=16, lr≈2e-5, epochs=2–3, early stopping.\n",
    "\n",
    "DEPRESSION_PATH = \"data/depression_dataset_reddit_cleaned.csv\"\n",
    "TDT_SPLIT = \"80/10/10\"\n",
    "BERT_MODEL = 'distilbert-base-uncased'\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e686069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing 80/10/10 split\n",
      "Completing 80/10/10 split\n"
     ]
    }
   ],
   "source": [
    "data, labels = helpers.get_data()\n",
    "train_text, dev_text, test_text = helpers.split(data=data, dist=TDT_SPLIT)\n",
    "train_dep, dev_dep, test_dep = helpers.split(data=labels, dist=TDT_SPLIT)\n",
    "\n",
    "# convert data, labels into a pandas DataFrame\n",
    "df = pd.DataFrame({'clean_text': data, 'is_depression': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7ffd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL)\n",
    "bert = DistilBertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ffeec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length is :  30522\n",
      "network\n",
      "first 10: ['[PAD]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]']\n",
      "1280-1290: ['ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ']\n",
      "last 10: ['##！', '##（', '##）', '##，', '##－', '##．', '##／', '##：', '##？', '##～']\n",
      "token 2897: network\n",
      "token 102: [SEP]\n",
      "token 103: [MASK]\n",
      "alex id: 4074\n"
     ]
    }
   ],
   "source": [
    "# stats from tokenizers\n",
    "print(\"Vocab length is : \",len(tokenizer.vocab))\n",
    "print(tokenizer.decode(2897))\n",
    "\n",
    "vocab = list(tokenizer.vocab.keys())\n",
    "print(f\"first 10: {vocab[:10]}\")\n",
    "print(f\"1280-1290: {vocab[1280:1290]}\")\n",
    "print(f\"last 10: {vocab[-10:]}\")\n",
    "\n",
    "print(f\"token 2897: {tokenizer.decode(2897)}\")\n",
    "print(f\"token 102: {tokenizer.decode(102)}\")\n",
    "print(f\"token 103: {tokenizer.decode(103)}\")\n",
    "if 'alex' in tokenizer.vocab:\n",
    "  print(f\"alex id: {tokenizer.vocab['alex']}\")\n",
    "else:\n",
    "  print(\"alex not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482facdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize train, dev, and test\n",
    "train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "dev_encodings = tokenizer(dev_text, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ea90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class\n",
    "class DepressionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e855a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_dep)\n",
    "dev_labels = torch.tensor(dev_dep)\n",
    "test_labels = torch.tensor(test_dep)\n",
    "\n",
    "# Create datasets using the custom class\n",
    "train_dataset = DepressionDataset(train_encodings, train_labels)\n",
    "dev_dataset = DepressionDataset(dev_encodings, dev_labels)\n",
    "test_dataset = DepressionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Create DataLoaders (for manual training if needed)\n",
    "dl_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_dev = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dl_test = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74de0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd08a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions),\n",
    "        'recall': recall_score(labels, predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e513ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/griffinmichalak/Desktop/code/fall25/nlp/nlp-final-project/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1161' max='1161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1161/1161 09:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.093319</td>\n",
       "      <td>0.971539</td>\n",
       "      <td>0.971204</td>\n",
       "      <td>0.997312</td>\n",
       "      <td>0.946429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055100</td>\n",
       "      <td>0.077907</td>\n",
       "      <td>0.981889</td>\n",
       "      <td>0.982005</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.974490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.081369</td>\n",
       "      <td>0.981889</td>\n",
       "      <td>0.982051</td>\n",
       "      <td>0.987113</td>\n",
       "      <td>0.977041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/griffinmichalak/Desktop/code/fall25/nlp/nlp-final-project/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/griffinmichalak/Desktop/code/fall25/nlp/nlp-final-project/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1161, training_loss=0.07839657939989746, metrics={'train_runtime': 595.8979, 'train_samples_per_second': 31.133, 'train_steps_per_second': 1.948, 'total_flos': 1228767589933056.0, 'train_loss': 0.07839657939989746, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# begin training\n",
    "# using hugging face built in trainer https://huggingface.co/blog/davidberenstein1957/fine-tune-modernbert-on-synthetic-data#:~:text=the%20training%20arguments.-,from%20huggingface_hub%20import%20HfFolder,-from%20transformers%20import\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-depression-classifier\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
